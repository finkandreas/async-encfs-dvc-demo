# PyTorch Vision Transformer description for DVC stage generation

app:
  name: &app_name ex_vit/cifar10/baseline_model  # apps should always be versioned, dataset can be absorbed into model in case of redundancy
  code_root: &code_root "\\$(git rev-parse --show-toplevel)/examples/ex_vit"  # /src/app

  # defaults for SLURM, can be overriden in merged mappings
  slurm_defaults: &slurm_defaults
    # environment configuration in sbatch script before srun (only stage supported)
    stage_env: |
      module load daint-gpu
      module load PyTorch
      export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

      # Environment variables needed by the NCCL backend for distributed training
      export NCCL_DEBUG=INFO
      export NCCL_IB_HCA=ipogif0
      export NCCL_IB_CUDA_SUPPORT=1
    dvc:  # sbatch options
      --cpus-per-task: 24
      --constraint: mc
      --time: '4:00:00'
    all:  # sbatch options
      --account: csstaff

  stages: # app-specific stages
    inference:
      type: ml_inference_stage # refers to included stage definition
      script: [*code_root, inference.py]
      input_inference: # parameterizes stage
        name: &input_inference_app_name in/cifar10
        stage: &input_inference_app_stage original
      extra_command_line_options:
        --dry-run: ~
        # --no-cuda: ~

      slurm_opts:  # run with SLURM
        <<: *slurm_defaults
        stage:  # sbatch options
          --nodes: 1
          --ntasks: 1
          --cpus-per-task: 12
          --constraint: gpu
          --time: '4:00:00'


# include dvc-root/mounts and stage type information (paths relative to DVC root)
include:
  dvc_root: '.dvc_policies/repo/dvc_root.yaml'
  ml_inference_stage: '.dvc_policies/stages/dvc_ml_inference.yaml'

# DVC repo configuration with encfs-encryption

host_data:
  dvc_root: &dvc_root_host ../..  # output of `dvc root`
  dvc_config: &dvc_config_host config
  mount:  # relative to dvc_root_host
    data:
      type: encfs
      origin: encrypt
      default_target: &mount_data_host decrypt  # make sure this is a dvc-repo-specific path if it is absolute
      custom_target:  # machine-specific
        - machine: ['daint[\d]+', 'nid[\d]+'] # TODO: Alps
          target: /tmp/encfs_$(id -u)_async_encfs_dvc  # make sure this is a dvc-repo-specific path if using multiple encfs-repos

# A generic ML-inference stage
# All values are interpreted as paths relative to the host/container data (encfs) mount point

ml_inference_stage:  # stage_data is relative to mount data point
  input:  # input data dependencies
    training:  # trained model
      stage_data: &input_trained_model [*app_name, training, "run_20231004_164512", output]
      command_line_options:  # for script
        --training-output: [*input_trained_model, ""]

    inference:  # inference sample/batch
      stage_data: &input_inference [*input_inference_app_name, *input_inference_app_stage, ".", output]
      command_line_options:
        --inference-input: [ *input_inference, "" ]

    config:  # hyperparameter config
      stage_data: &input_config [*app_name, config, "default", output]
      command_line_options:
        --config: [*input_config, "config.yaml"]

  output:  # inference output
    inference:
      stage_data: &output_inference [*app_name, inference, "run_20231004_165212", output]
      command_line_options:
        --inference-output: *output_inference

  dvc: [*output_inference, ".."]  # dvc.yaml storage location

original:
  file: "$(dvc root)/../../ex_vit/23-10-04_16-05-31_daint102_anfink_dvc_app.yaml"  # source of this DVC app stage configuration
  run_label: "run_20231004_165212"  # original run_label used

